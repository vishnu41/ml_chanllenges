{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('labeledTrainData.tsv',sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>[stuff, going, moment, mj, ive, started, liste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  [stuff, going, moment, mj, ive, started, liste...\n",
       "1  2381_9          1  [classic, war, worlds, timothy, hines, enterta...\n",
       "2  7759_3          0  [film, starts, manager, nicholas, bell, giving...\n",
       "3  3630_4          0  [must, assumed, praised, film, greatest, filme...\n",
       "4  9495_8          1  [superbly, trashy, wondrously, unpretentious, ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "getPunctuations = str.maketrans(\"\",\"\",string.punctuation) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "train_df['review'] = train_df['review'].apply(lambda x: x.lower())\n",
    "train_df['review'] = train_df['review'].apply(lambda x: x.translate(getPunctuations))\n",
    "train_df['review'] = train_df['review'].apply(lambda x: x.split())\n",
    "train_df['review'] = train_df['review'].apply(lambda x: [item for item in x if item not in stop])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_words = 10000\n",
    "max_len = 200\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['review'].values\n",
    "sentences = list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have model with words embedded. We can query model for similar words like below or ask to represent word as vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7405692\n",
      "[('film', 0.8293483257293701), ('moviebr', 0.8118131160736084)]\n",
      "movie 0.8293483257293701\n",
      "filmbr 0.762588381767273\n",
      "films 0.7148263454437256\n",
      "moviebr 0.688957691192627\n",
      "documentary 0.6654255390167236\n",
      "picture 0.6549851894378662\n",
      "cinema 0.6480210423469543\n",
      "product 0.6471505761146545\n",
      "experience 0.6226150989532471\n",
      "certainly 0.618864119052887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coe04\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\Users\\coe04\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\coe04\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#to get similarity between words\n",
    "print(model.similarity('good','great'))\n",
    "\n",
    "print (model.most_similar(positive=['movie'], negative=[], topn=2))\n",
    "\n",
    "#get the words most similar to given word in our data\n",
    "ms=model.most_similar('film')\n",
    "for x in ms:\n",
    "    print (x[0],x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get vocabulary or the number of words in vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121223\n"
     ]
    }
   ],
   "source": [
    "# print(list(model.wv.vocab))\n",
    "print (len(list(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will feed word embeddings into clustering algorithm such as k Means which is one of the most popular unsupervised learning algorithms for finding interesting segments in the data. It can be used for separating customers into groups, combining documents into topics and for many other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "preparing data for kmeans clustering (converting each sentence into vectors, so that clustering occurs based on sentences). We are taking sum of vectors of each word and averaging over length of sentence. even though aeraging word vectors doesn't have any meaning, we are getting accuracy of 63%. We should find a way to use original word embeddings for text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "  \n",
    "  \n",
    "X=[]\n",
    "for sentence in sentences:\n",
    "    X.append(sent_vectorizer(sentence, model))   \n",
    " \n",
    "print (\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print (len(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means Clustering with Scikit-learn Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on k means from scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=20, n_jobs=2, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2,n_jobs = 2,init = \"k-means++\",n_init=20,random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id labels for inputted data\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "y_pred = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (len(y_pred))\n",
    "\n",
    "# print(\"Top terms per cluster:\")\n",
    "# order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = list(model.wv.vocab)\n",
    "# for i in range(2):\n",
    "#     print(\"Cluster %d:\" % i),\n",
    "#     for ind in order_centroids[i, :10]:\n",
    "#         print(' %s' % terms[ind]),\n",
    "#     print\n",
    "    \n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means Clustering with NLTK Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "NUM_CLUSTERS=2\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print (len(assigned_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:['stuff', 'going', 'moment', 'mj', 'ive', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mjs', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'mkaybr', 'br', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'himbr', 'br', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', '20', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pescis', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mjs', 'musicbr', 'br', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scenebr', 'br', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mjs', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'ive', 'gave', 'subjecthmmm', 'well', 'dont', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n",
      "0:['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'every', 'year', 'eg', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'everyone', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'critics', 'look', 'criticize', 'everything', 'others', 'rate', 'movie', 'important', 'baseslike', 'entertained', 'people', 'never', 'agree', 'critics', 'enjoyed', 'effort', 'mr', 'hines', 'put', 'faithful', 'hg', 'wells', 'classic', 'novel', 'found', 'entertaining', 'made', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings']\n",
      "1:['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', '¨jurassik', 'park¨', 'scientists', 'resurrect', 'one', 'natures', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'however', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'scientificmeanwhile', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'prehistorical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'though', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'one', 'natures', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalks', 'victimsbr', 'br', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hairraising', 'chillsfull', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'effectsthe', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'made', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middling', 'performances', 'though', 'players', 'reacting', 'appropriately', 'becoming', 'foodactors', 'give', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'runningbound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realisticgory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', '¨sabretooth2002¨by', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'much', 'better', '¨10000', 'bc2006¨', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'many', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'many', 'others', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxannerobinson', 'crusoe', 'rating', 'average', 'bottom', 'barrel']\n",
      "0:['must', 'assumed', 'praised', 'film', 'greatest', 'filmed', 'opera', 'ever', 'didnt', 'read', 'somewhere', 'either', 'dont', 'care', 'opera', 'dont', 'care', 'wagner', 'dont', 'care', 'anything', 'except', 'desire', 'appear', 'cultured', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strikes', 'unmitigated', 'disaster', 'leaden', 'reading', 'score', 'matched', 'tricksy', 'lugubrious', 'realisation', 'textbr', 'br', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allowed', 'anywhere', 'near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decided', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter', 'stages', 'transmutes', 'kind', 'beatnik', 'babe', 'though', 'one', 'continues', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double', 'dose', 'armin', 'jordan', 'conductor', 'seen', 'face', 'heard', 'voice', 'amfortas', 'also', 'appears', 'monstrously', 'double', 'exposure', 'kind', 'batonzilla', 'conductor', 'ate', 'monsalvat', 'playing', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represented', 'scattering', 'shopworn', 'flaccid', 'crocuses', 'stuck', 'illlaid', 'turf', 'expedient', 'baffles', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provided', 'julie', 'andrews', 'sound', 'musicbr', 'br', 'sound', 'hard', 'endure', 'high', 'voices', 'trumpets', 'particular', 'possessing', 'aural', 'glare', 'adds', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conducting', 'paralytic', 'unfolding', 'ritual', 'someone', 'another', 'review', 'mentioned', '1951', 'bayreuth', 'recording', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lacks', 'sense', 'pulse', 'feeling', 'ebb', 'flow', 'music', 'half', 'century', 'orchestral', 'sound', 'set', 'modern', 'pressings', 'still', 'superior', 'film']\n",
      "1:['superbly', 'trashy', 'wondrously', 'unpretentious', '80s', 'exploitation', 'hooray', 'precredits', 'opening', 'sequences', 'somewhat', 'give', 'false', 'impression', 'dealing', 'serious', 'harrowing', 'drama', 'need', 'fear', 'barely', 'ten', 'minutes', 'later', 'necks', 'nonsensical', 'chainsaw', 'battles', 'rough', 'fistfights', 'lurid', 'dialogs', 'gratuitous', 'nudity', 'bo', 'ingrid', 'two', 'orphaned', 'siblings', 'unusually', 'close', 'even', 'slightly', 'perverted', 'relationship', 'imagine', 'playfully', 'ripping', 'towel', 'covers', 'sisters', 'naked', 'body', 'stare', 'unshaven', 'genitals', 'several', 'whole', 'minutes', 'well', 'bo', 'sister', 'judging', 'dubbed', 'laughter', 'doesnt', 'mind', 'sick', 'dude', 'anyway', 'kids', 'fled', 'russia', 'parents', 'nasty', 'soldiers', 'brutally', 'slaughtered', 'mommy', 'daddy', 'friendly', 'smuggler', 'took', 'custody', 'however', 'even', 'raised', 'trained', 'bo', 'ingrid', 'expert', 'smugglers', 'actual', 'plot', 'lifts', '20', 'years', 'later', 'theyre', 'facing', 'ultimate', 'quest', 'mythical', 'incredibly', 'valuable', 'white', 'fire', 'diamond', 'coincidentally', 'found', 'mine', 'things', 'life', 'ever', 'made', 'little', 'sense', 'plot', 'narrative', 'structure', 'white', 'fire', 'sure', 'lot', 'fun', 'watch', 'time', 'clue', 'whos', 'beating', 'cause', 'bet', 'actors', 'understood', 'even', 'less', 'whatever', 'violence', 'magnificently', 'grotesque', 'every', 'single', 'plot', 'twist', 'pleasingly', 'retarded', 'script', 'goes', 'totally', 'bonkers', 'beyond', 'repair', 'suddenly', '\\x96', 'wont', 'reveal', 'reason', '\\x96', 'bo', 'needs', 'replacement', 'ingrid', 'fred', 'williamson', 'enters', 'scene', 'big', 'cigar', 'mouth', 'sleazy', 'black', 'fingers', 'local', 'prostitutes', 'bos', 'principal', 'opponent', 'italian', 'chick', 'big', 'breasts', 'hideous', 'accent', 'preposterous', 'catchy', 'theme', 'song', 'plays', 'least', 'dozen', 'times', 'throughout', 'film', 'theres', 'obligatory', 'werefallinginlove', 'montage', 'loads', 'attractions', 'god', 'brilliant', 'experience', 'original', 'french', 'title', 'translates', 'life', 'survive', 'uniquely', 'appropriate', 'makes', 'much', 'sense', 'rest', 'movie', 'none']\n",
      "0:['dont', 'know', 'people', 'think', 'bad', 'movie', 'got', 'pretty', 'good', 'plot', 'good', 'action', 'change', 'location', 'harry', 'hurt', 'either', 'sure', 'offensive', 'gratuitous', 'movie', 'like', 'eastwood', 'good', 'form', 'dirty', 'harry', 'liked', 'pat', 'hingle', 'movie', 'small', 'town', 'cop', 'liked', 'dirty', 'harry', 'see', 'one', 'lot', 'better', 'dead', 'pool', '45']\n",
      "0:['movie', 'could', 'good', 'comes', 'way', 'short', 'cheesy', 'special', 'effects', 'soso', 'acting', 'could', 'looked', 'past', 'story', 'wasnt', 'lousy', 'background', 'story', 'would', 'better', 'plot', 'centers', 'around', 'evil', 'druid', 'witch', 'linked', 'woman', 'gets', 'migraines', 'movie', 'drags', 'never', 'clearly', 'explains', 'anything', 'keeps', 'plodding', 'christopher', 'walken', 'part', 'completely', 'senseless', 'movie', 'movie', 'potential', 'looks', 'like', 'really', 'bad', 'made', 'tv', 'movie', 'would', 'avoid', 'movie']\n",
      "0:['watched', 'video', 'friends', 'house', 'im', 'glad', 'waste', 'money', 'buying', 'one', 'video', 'cover', 'scene', '1975', 'movie', 'capricorn', 'one', 'movie', 'starts', 'several', 'clips', 'rocket', 'blowups', 'related', 'manned', 'flight', 'sibrels', 'smoking', 'gun', 'short', 'video', 'clip', 'astronauts', 'preparing', 'video', 'broadcast', 'edits', 'voiceover', 'instead', 'letting', 'us', 'listen', 'crew', 'say', 'video', 'curiously', 'ends', 'showing', 'zapruder', 'film', 'claims', 'radiation', 'shielding', 'star', 'photography', 'others', 'lead', 'believe', 'extremely', 'ignorant', 'sort', 'ax', 'grind', 'nasa', 'astronauts', 'american', 'general', 'science', 'bad', 'video']\n",
      "1:['friend', 'mine', 'bought', 'film', '£1', 'even', 'grossly', 'overpriced', 'despite', 'featuring', 'big', 'names', 'adam', 'sandler', 'billy', 'bob', 'thornton', 'incredibly', 'talented', 'burt', 'young', 'film', 'funny', 'taking', 'chisel', 'hammering', 'straight', 'earhole', 'uses', 'tired', 'bottom', 'barrel', 'comedic', 'techniques', 'consistently', 'breaking', 'fourth', 'wall', 'sandler', 'talks', 'audience', 'seemingly', 'pointless', 'montages', 'hot', 'girlsbr', 'br', 'adam', 'sandler', 'plays', 'waiter', 'cruise', 'ship', 'wants', 'make', 'successful', 'comedian', 'order', 'become', 'successful', 'women', 'ships', 'resident', 'comedian', 'shamelessly', 'named', 'dickie', 'due', 'unfathomable', 'success', 'opposite', 'gender', 'presumed', 'lost', 'sea', 'sandlers', 'character', 'shecker', 'gets', 'big', 'break', 'dickie', 'dead', 'hes', 'rather', 'locked', 'bathroom', 'presumably', 'sea', 'sickbr', 'br', 'perhaps', 'mouth', 'vomited', 'worst', 'film', 'time']\n",
      "0:['br', 'br', 'movie', 'full', 'references', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'many', 'others', 'ladybug´s', 'face', 'it´s', 'clear', 'reference', 'tribute', 'peter', 'lorre', 'movie', 'masterpiece', 'we´ll', 'talk', 'much', 'future']\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for index, sentence in enumerate(sentences[:10]):    \n",
    "    print (str(assigned_clusters[index]) + \":\" + str(sentence))\n",
    "    y_pred.append(assigned_clusters[index])\n",
    "#     print(index),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50088\n",
      "[[12491     9]\n",
      " [12469    31]]\n",
      "0.0049441786283891545\n"
     ]
    }
   ],
   "source": [
    "y_true = train_df['sentiment'].values\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_true,y_pred))\n",
    "print(metrics.confusion_matrix(y_true,y_pred))\n",
    "print(metrics.f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since averaging word vectors doesn't have any geometric meaning, I'm trying doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    " \n",
    "print (common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]), TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]), TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]), TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]), TaggedDocument(words=['user', 'response', 'time'], tags=[4]), TaggedDocument(words=['trees'], tags=[5]), TaggedDocument(words=['graph', 'trees'], tags=[6]), TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]), TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])]\n"
     ]
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    " \n",
    "print (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coe04\\AppData\\Local\\Temp\\my_doc2vec_model\n"
     ]
    }
   ],
   "source": [
    "#Persist a model to disk:\n",
    " \n",
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    " \n",
    "print (fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from saved file\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # you can continue training with the loaded model!\n",
    "# #If you’re finished training a model (=no more updates, only querying, reduce memory usage), you can do:\n",
    " \n",
    "# model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    " \n",
    "# #Infer vector for a new document:\n",
    "# #Here our text paragraph just 2 words\n",
    "# vector = model.infer_vector([\"system\", \"response\"])\n",
    "# print (vector)\n",
    " \n",
    "# \"\"\"\n",
    "# output\n",
    " \n",
    "# [-0.08390492  0.01629403 -0.08274432  0.06739668 -0.07021132]\n",
    "  \n",
    "#  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    " \n",
    "import gensim.models as g\n",
    "import codecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stuff', 'going', 'moment', 'mj', 'ive', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mjs', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'mkaybr', 'br', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'himbr', 'br', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', '20', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pescis', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mjs', 'musicbr', 'br', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scenebr', 'br', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mjs', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'ive', 'gave', 'subjecthmmm', 'well', 'dont', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter'], ['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'every', 'year', 'eg', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'everyone', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'critics', 'look', 'criticize', 'everything', 'others', 'rate', 'movie', 'important', 'baseslike', 'entertained', 'people', 'never', 'agree', 'critics', 'enjoyed', 'effort', 'mr', 'hines', 'put', 'faithful', 'hg', 'wells', 'classic', 'novel', 'found', 'entertaining', 'made', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings'], ['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', '¨jurassik', 'park¨', 'scientists', 'resurrect', 'one', 'natures', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'however', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'scientificmeanwhile', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'prehistorical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'though', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'one', 'natures', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalks', 'victimsbr', 'br', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hairraising', 'chillsfull', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'effectsthe', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'made', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middling', 'performances', 'though', 'players', 'reacting', 'appropriately', 'becoming', 'foodactors', 'give', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'runningbound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realisticgory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', '¨sabretooth2002¨by', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'much', 'better', '¨10000', 'bc2006¨', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'many', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'many', 'others', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxannerobinson', 'crusoe', 'rating', 'average', 'bottom', 'barrel'], ['must', 'assumed', 'praised', 'film', 'greatest', 'filmed', 'opera', 'ever', 'didnt', 'read', 'somewhere', 'either', 'dont', 'care', 'opera', 'dont', 'care', 'wagner', 'dont', 'care', 'anything', 'except', 'desire', 'appear', 'cultured', 'either', 'representation', 'wagners', 'swansong', 'movie', 'strikes', 'unmitigated', 'disaster', 'leaden', 'reading', 'score', 'matched', 'tricksy', 'lugubrious', 'realisation', 'textbr', 'br', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allowed', 'anywhere', 'near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagners', 'text', 'decided', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter', 'stages', 'transmutes', 'kind', 'beatnik', 'babe', 'though', 'one', 'continues', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double', 'dose', 'armin', 'jordan', 'conductor', 'seen', 'face', 'heard', 'voice', 'amfortas', 'also', 'appears', 'monstrously', 'double', 'exposure', 'kind', 'batonzilla', 'conductor', 'ate', 'monsalvat', 'playing', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represented', 'scattering', 'shopworn', 'flaccid', 'crocuses', 'stuck', 'illlaid', 'turf', 'expedient', 'baffles', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'cant', 'think', 'syberberg', 'couldnt', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provided', 'julie', 'andrews', 'sound', 'musicbr', 'br', 'sound', 'hard', 'endure', 'high', 'voices', 'trumpets', 'particular', 'possessing', 'aural', 'glare', 'adds', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conducting', 'paralytic', 'unfolding', 'ritual', 'someone', 'another', 'review', 'mentioned', '1951', 'bayreuth', 'recording', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lacks', 'sense', 'pulse', 'feeling', 'ebb', 'flow', 'music', 'half', 'century', 'orchestral', 'sound', 'set', 'modern', 'pressings', 'still', 'superior', 'film'], ['superbly', 'trashy', 'wondrously', 'unpretentious', '80s', 'exploitation', 'hooray', 'precredits', 'opening', 'sequences', 'somewhat', 'give', 'false', 'impression', 'dealing', 'serious', 'harrowing', 'drama', 'need', 'fear', 'barely', 'ten', 'minutes', 'later', 'necks', 'nonsensical', 'chainsaw', 'battles', 'rough', 'fistfights', 'lurid', 'dialogs', 'gratuitous', 'nudity', 'bo', 'ingrid', 'two', 'orphaned', 'siblings', 'unusually', 'close', 'even', 'slightly', 'perverted', 'relationship', 'imagine', 'playfully', 'ripping', 'towel', 'covers', 'sisters', 'naked', 'body', 'stare', 'unshaven', 'genitals', 'several', 'whole', 'minutes', 'well', 'bo', 'sister', 'judging', 'dubbed', 'laughter', 'doesnt', 'mind', 'sick', 'dude', 'anyway', 'kids', 'fled', 'russia', 'parents', 'nasty', 'soldiers', 'brutally', 'slaughtered', 'mommy', 'daddy', 'friendly', 'smuggler', 'took', 'custody', 'however', 'even', 'raised', 'trained', 'bo', 'ingrid', 'expert', 'smugglers', 'actual', 'plot', 'lifts', '20', 'years', 'later', 'theyre', 'facing', 'ultimate', 'quest', 'mythical', 'incredibly', 'valuable', 'white', 'fire', 'diamond', 'coincidentally', 'found', 'mine', 'things', 'life', 'ever', 'made', 'little', 'sense', 'plot', 'narrative', 'structure', 'white', 'fire', 'sure', 'lot', 'fun', 'watch', 'time', 'clue', 'whos', 'beating', 'cause', 'bet', 'actors', 'understood', 'even', 'less', 'whatever', 'violence', 'magnificently', 'grotesque', 'every', 'single', 'plot', 'twist', 'pleasingly', 'retarded', 'script', 'goes', 'totally', 'bonkers', 'beyond', 'repair', 'suddenly', '\\x96', 'wont', 'reveal', 'reason', '\\x96', 'bo', 'needs', 'replacement', 'ingrid', 'fred', 'williamson', 'enters', 'scene', 'big', 'cigar', 'mouth', 'sleazy', 'black', 'fingers', 'local', 'prostitutes', 'bos', 'principal', 'opponent', 'italian', 'chick', 'big', 'breasts', 'hideous', 'accent', 'preposterous', 'catchy', 'theme', 'song', 'plays', 'least', 'dozen', 'times', 'throughout', 'film', 'theres', 'obligatory', 'werefallinginlove', 'montage', 'loads', 'attractions', 'god', 'brilliant', 'experience', 'original', 'french', 'title', 'translates', 'life', 'survive', 'uniquely', 'appropriate', 'makes', 'much', 'sense', 'rest', 'movie', 'none'], ['dont', 'know', 'people', 'think', 'bad', 'movie', 'got', 'pretty', 'good', 'plot', 'good', 'action', 'change', 'location', 'harry', 'hurt', 'either', 'sure', 'offensive', 'gratuitous', 'movie', 'like', 'eastwood', 'good', 'form', 'dirty', 'harry', 'liked', 'pat', 'hingle', 'movie', 'small', 'town', 'cop', 'liked', 'dirty', 'harry', 'see', 'one', 'lot', 'better', 'dead', 'pool', '45'], ['movie', 'could', 'good', 'comes', 'way', 'short', 'cheesy', 'special', 'effects', 'soso', 'acting', 'could', 'looked', 'past', 'story', 'wasnt', 'lousy', 'background', 'story', 'would', 'better', 'plot', 'centers', 'around', 'evil', 'druid', 'witch', 'linked', 'woman', 'gets', 'migraines', 'movie', 'drags', 'never', 'clearly', 'explains', 'anything', 'keeps', 'plodding', 'christopher', 'walken', 'part', 'completely', 'senseless', 'movie', 'movie', 'potential', 'looks', 'like', 'really', 'bad', 'made', 'tv', 'movie', 'would', 'avoid', 'movie'], ['watched', 'video', 'friends', 'house', 'im', 'glad', 'waste', 'money', 'buying', 'one', 'video', 'cover', 'scene', '1975', 'movie', 'capricorn', 'one', 'movie', 'starts', 'several', 'clips', 'rocket', 'blowups', 'related', 'manned', 'flight', 'sibrels', 'smoking', 'gun', 'short', 'video', 'clip', 'astronauts', 'preparing', 'video', 'broadcast', 'edits', 'voiceover', 'instead', 'letting', 'us', 'listen', 'crew', 'say', 'video', 'curiously', 'ends', 'showing', 'zapruder', 'film', 'claims', 'radiation', 'shielding', 'star', 'photography', 'others', 'lead', 'believe', 'extremely', 'ignorant', 'sort', 'ax', 'grind', 'nasa', 'astronauts', 'american', 'general', 'science', 'bad', 'video'], ['friend', 'mine', 'bought', 'film', '£1', 'even', 'grossly', 'overpriced', 'despite', 'featuring', 'big', 'names', 'adam', 'sandler', 'billy', 'bob', 'thornton', 'incredibly', 'talented', 'burt', 'young', 'film', 'funny', 'taking', 'chisel', 'hammering', 'straight', 'earhole', 'uses', 'tired', 'bottom', 'barrel', 'comedic', 'techniques', 'consistently', 'breaking', 'fourth', 'wall', 'sandler', 'talks', 'audience', 'seemingly', 'pointless', 'montages', 'hot', 'girlsbr', 'br', 'adam', 'sandler', 'plays', 'waiter', 'cruise', 'ship', 'wants', 'make', 'successful', 'comedian', 'order', 'become', 'successful', 'women', 'ships', 'resident', 'comedian', 'shamelessly', 'named', 'dickie', 'due', 'unfathomable', 'success', 'opposite', 'gender', 'presumed', 'lost', 'sea', 'sandlers', 'character', 'shecker', 'gets', 'big', 'break', 'dickie', 'dead', 'hes', 'rather', 'locked', 'bathroom', 'presumably', 'sea', 'sickbr', 'br', 'perhaps', 'mouth', 'vomited', 'worst', 'film', 'time'], ['br', 'br', 'movie', 'full', 'references', 'like', 'mad', 'max', 'ii', 'wild', 'one', 'many', 'others', 'ladybug´s', 'face', 'it´s', 'clear', 'reference', 'tribute', 'peter', 'lorre', 'movie', 'masterpiece', 'we´ll', 'talk', 'much', 'future']]\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "# m = Doc2Vec.load(model)\n",
    "\n",
    "print (sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for d in sentences:\n",
    "    X.append( model.infer_vector(d, alpha=start_alpha, steps=infer_epoch) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=2,\n",
       "   threshold=0.1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=2\n",
    " \n",
    "from sklearn.cluster import Birch\n",
    " \n",
    "brc = Birch(branching_factor=50, n_clusters=k, threshold=0.1, compute_labels=True)\n",
    "brc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: \n",
      "25000\n",
      "Silhouette_score: \n",
      "0.76188165\n"
     ]
    }
   ],
   "source": [
    "clusters = brc.predict(X)\n",
    " \n",
    "y_pred = brc.labels_\n",
    " \n",
    " \n",
    "print (\"Clusters: \")\n",
    "print (len(clusters))\n",
    " \n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']),\n",
       " TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coe04\\Miniconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  segments.append('\"%s\"' % self.comment)\n",
      "C:\\Users\\coe04\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 0.01357531  0.00292198 -0.01307619 -0.00687181 -0.0229191   0.00622976\n",
      " -0.02385645 -0.00078574 -0.01658066  0.00437424 -0.00253913  0.00128016\n",
      "  0.01728363  0.0206915  -0.00381131  0.03159789  0.01927276  0.01060365\n",
      " -0.0030602   0.01160008]\n",
      "[('0', 0.993676483631134), ('2', 0.9932615756988525), ('3', 0.9922387599945068)]\n",
      "[-0.01233657  0.06075326  0.04266645  0.20951614 -0.23930328  0.13669424\n",
      "  0.02943046  0.3629795   0.3319709  -0.29642266  0.12508218 -0.05567428\n",
      "  0.07741094  0.17015728 -0.1505286   0.45622203  0.38881418 -0.47629637\n",
      " -0.3589217  -0.3327939 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
